import { useCallback, useEffect, useRef, useState } from "react";
import {
  ExpoAudioStreamModule,
  useAudioRecorder,
  AudioDataEvent,
} from "@siteed/expo-audio-studio";
import { useSocket } from "./useSocket";

interface UseAudioServiceReturn {
  isRecording: boolean;
  startRecording: () => Promise<void>;
  stopRecording: () => Promise<void>;
  audioLevel: number;
  sampleRate: number;
  bufferSize: number;
  sttDatas: STTData[];
}

interface STTData {
  clientId: string;
  isFinal: boolean;
  resultId: string;
  text: string;
  timestamp: string;
}

export const useStreamingAudioService = (): UseAudioServiceReturn => {
  const [isRecordingState, setIsRecordingState] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);
  const [sampleRate, setSampleRate] = useState(0);
  const [bufferSize, setBufferSize] = useState(0);

  const [sttDatas, setSttDatas] = useState<STTData[]>([]);

  const onAudioDataRef = useRef<(event: AudioDataEvent) => Promise<void>>(
    async () => {}
  );
  const audioChunksRef = useRef<string[]>([]);
  const webAudioChunksRef = useRef<Float32Array>(new Float32Array(0));
  const currentSizeRef = useRef(0);
  const [socketId, setSocketId] = useState<string | null>(null);

  const {
    startRecording: startRecordingNative,
    stopRecording: stopRecordingNative,
  } = useAudioRecorder();

  const {
    sendAudioChunk: sendAudioChunkViaSocket,
    connect: connectSocket,
    disconnect: disconnectSocket,
  } = useSocket({
    socketUrl: process.env.EXPO_PUBLIC_API_URL!,
    onTranscriptionResult: (data) => {
      console.log(data);
      setSttDatas((prev) => {
        const exists = prev.find((d) => d.resultId === data.resultId);
        if (exists) {
          return prev.map((d) => (d.resultId === data.resultId ? data : d));
        } else {
          return [...prev, data];
        }
      });
    },
    onTranscriptionError: (error) => {
      console.error("Transcription error handled:", error);
    },
    onConnect: (s) => setSocketId(s.id!),
  });

  // Ïò§ÎîîÏò§ Ï≤≠ÌÅ¨ Ï†ÑÏÜ° wrapper
  const sendAudioChunk = useCallback(
    (audioData: string, soundLevel: number) => {
      sendAudioChunkViaSocket(audioData, soundLevel);
    },
    [sendAudioChunkViaSocket]
  );

  // Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Ìï∏Îì§Îü¨
  const setupAudioDataHandler = useCallback(() => {
    onAudioDataRef.current = async (event: AudioDataEvent): Promise<void> => {
      try {
        const { data, eventDataSize } = event;
        console.debug(
          `üé§ Processing audio data: type=${typeof data}, size=${eventDataSize}`
        );

        if (!eventDataSize || eventDataSize === 0) return;

        if (typeof data === "string") {
          console.debug("üì± Processing string data (Native platform)");
          if (audioChunksRef.current) {
            audioChunksRef.current.push(data);
          }
          sendAudioChunk(data, 0);
        } else if (data instanceof Float32Array) {
          console.debug("üåê Processing Float32Array data (Web platform)");

          // Float32ArrayÎ•º PCM 16-bitÎ°ú Î≥ÄÌôò ÌõÑ Base64 Ïù∏ÏΩîÎî©
          const pcmData = new Int16Array(data.length);
          for (let i = 0; i < data.length; i++) {
            const sample = Math.max(-1, Math.min(1, data[i]));
            pcmData[i] = sample < 0 ? sample * 0x8000 : sample * 0x7fff;
          }

          const uint8Array = new Uint8Array(pcmData.buffer);
          const base64String = btoa(String.fromCharCode(...uint8Array));
          sendAudioChunk(base64String, 0);
        } else if (
          ArrayBuffer.isView(data) &&
          (data as any).BYTES_PER_ELEMENT === 2
        ) {
          // Int16Array Ï≤òÎ¶¨ - Ïù¥ÎØ∏ PCM 16-bit Ìè¨Îß∑Ïù¥ÎØÄÎ°ú ÏßÅÏ†ë Base64Î°ú Î≥ÄÌôò
          console.debug("Processing Int16Array data");

          const int16Data = data as Int16Array;
          const uint8Array = new Uint8Array(int16Data.buffer);
          const base64String = btoa(String.fromCharCode(...uint8Array));
          sendAudioChunk(base64String, 0);
        }
      } catch (error) {
        console.error("Error processing audio data:", error);
      }
      return Promise.resolve();
    };
  }, [sendAudioChunk]);

  // ÎÖπÏùå ÏãúÏûë
  const startRecording = useCallback(async () => {
    try {
      const { granted } = await ExpoAudioStreamModule.requestPermissionsAsync();
      if (!granted) {
        console.log("Microphone permissions denied");
        return;
      }

      setupAudioDataHandler();

      webAudioChunksRef.current = new Float32Array(0);
      audioChunksRef.current = [];
      currentSizeRef.current = 0;

      await startRecordingNative({
        sampleRate: 16000,
        onAudioStream: (event: AudioDataEvent): Promise<void> =>
          onAudioDataRef.current(event),
      });

      setSampleRate(16000);
      setBufferSize(1024);
      setIsRecordingState(true);

      console.log("Recording started with Socket.IO streaming", {
        socketId: socketId,
        sampleRate: 16000,
      });
    } catch (error) {
      console.error("Failed to start recording:", error);
      throw error;
    }
  }, [setupAudioDataHandler, connectSocket]);

  // ÎÖπÏùå Ï§ëÏßÄ
  const stopRecording = useCallback(async () => {
    try {
      await stopRecordingNative();
      setIsRecordingState(false);

      audioChunksRef.current = [];
      webAudioChunksRef.current = new Float32Array(0);
      setAudioLevel(0);

      console.log("Recording stopped and Socket.IO disconnected");
    } catch (error) {
      console.error("Failed to stop recording:", error);
      throw error;
    }
  }, [disconnectSocket]);

  useEffect(() => {
    connectSocket();
    return () => {
      stopRecordingNative();
      disconnectSocket();
      audioChunksRef.current = [];
      webAudioChunksRef.current = new Float32Array(0);
    };
  }, []);

  return {
    isRecording: isRecordingState,
    startRecording,
    stopRecording,
    audioLevel,
    sampleRate,
    bufferSize,
    sttDatas,
  };
};
