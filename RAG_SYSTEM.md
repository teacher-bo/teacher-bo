# RAG 시스템 아키텍처 및 최적화

Teacher Bo의 RAG(Retrieval-Augmented Generation) 시스템은 보드게임 룰북을 기반으로 사용자의 질문에 정확하게 답변하기 위해 구축되었습니다. 시스템은 크게 **데이터 전처리 파이프라인**과 **실시간 질의응답 파이프라인**으로 구성됩니다.

## 1. 데이터 전처리 및 Vector DB 구축

정확한 답변을 위해서는 원본 룰북 데이터를 LLM이 이해하기 쉬운 형태로 가공하여 저장하는 것이 필수적입니다.

### 1-1. 전처리 파이프라인
1.  **텍스트 추출 (Text Extraction)**:
    *   초기에는 Upstage OCR을 사용했으나, 룰북의 불규칙한 레이아웃과 도표/그림 포함 특성을 고려하여 **LLM(Gemini 1.5 Pro)**을 활용한 텍스트 추출로 고도화하였습니다. 이를 통해 페이지의 맥락을 유지하며 텍스트를 추출할 수 있었습니다.
2.  **데이터 구조화 (Structuring)**:
    *   단순 텍스트 분할(Chunking) 대신, 룰북의 섹션과 의미 단위로 데이터를 구조화하여 **JSON** 형태로 관리합니다.
    *   특히, **Semantic Gap(의미적 차이)** 문제를 해결하기 위해 예상되는 질문-답변(QA) 쌍을 생성하여 함께 임베딩합니다.
3.  **임베딩 (Embedding)**:
    *   한국어 성능이 우수한 **Upstage Solar Embedding** 모델을 사용하여 텍스트를 벡터화합니다.
4.  **Vector DB 저장**:
    *   **ChromaDB**를 사용하여 임베딩된 벡터를 저장합니다. 로컬 환경에서 관리가 용이하고 LangChain과의 연동성이 뛰어납니다.

### 1-2. 데이터 구조 예시

RAG 성능을 극대화하기 위해 두 가지 형태의 데이터를 함께 활용합니다.

#### A. 공식 룰북 데이터 (Rulebook JSON)
룰북의 원문 내용을 섹션별로 구조화한 데이터입니다.

```json
[
  {
    "page": 1,
    "section_title": "내용물",
    "content": "타일 106개(1-13까지의 4가지 색깔 타일 각각 2세트, 조커 타일 2개), 받침대 4개",
    "type": "rulebook"
  },
  {
    "page": 1,
    "section_title": "게임의 목표",
    "content": "자신이 가진 모든 타일을 테이블에 연속이나 그룹 세트로 가장 먼저 내려놓기.",
    "type": "rulebook"
  }
]
```

#### B. QA 데이터 (QA JSON) - Semantic Gap 해결
사용자의 구어체 질문과 딱딱한 룰북 문장 사이의 간극을 줄이기 위해 구축한 데이터입니다.
예를 들어, *"모두가 광부인 경우가 있어?"*라는 질문은 룰북의 *"방해꾼이 없는 경우도 있을 수 있다"*는 문장과 매칭되기 어렵습니다. 이러한 문제를 해결하기 위해 QA 쌍을 추가하여 검색 정확도를 높였습니다.

```json
[
  {
    "question": "루미큐브에는 타일이 총 몇 개 들어있나요?",
    "answer": "기본 루미큐브에는 총 106개의 타일(1-13 숫자 타일 4색 2세트, 조커 2개)이 들어있습니다.",
    "page": 1,
    "section_title": "내용물",
    "content": "타일 106개(1-13까지의 4가지 색깔 타일 각각 2세트, 조커 타일 2개), 받침대 4개",
    "type": "QA"
  },
  {
    "question": "등록을 안 하고 바로 남의 타일에 붙일 수 있나요?",
    "answer": "불가능합니다. 첫 등록을 마친 후인 다음 차례부터 테이블의 타일을 이용할 수 있습니다.",
    "page": 1,
    "section_title": "게임 규칙",
    "content": "첫 등록을 하고 난 다음 자신의 차례부터는 테이블에 내려진 타일을 이용하여 자신의 타일을 내려놓을 수 있다.",
    "type": "QA"
  }
]
```

---

## 2. 성능 향상 및 최적화 전략

### 2-1. 프롬프트 엔지니어링
*   **구조화된 프롬프트**: Gemini의 프롬프트 가이드를 참고하여, LLM이 사고 과정을 단계별로 수행할 수 있도록 지시사항을 명확히 구조화했습니다.
*   **환각(Hallucination) 방지**: 주어진 Context 내에서 근거를 찾을 수 없는 경우, 무리하게 답변하지 않고 "알 수 없음"으로 응답하도록 엄격한 제약 조건을 설정했습니다.

### 2-2. 파라미터 최적화 테스트
실제 사용자 질문 데이터셋(25개)을 기반으로 다양한 파라미터 조합을 테스트하여 최적의 설정을 도출했습니다.

| 파라미터 | 최종 설정값 | 선정 이유 |
| :--- | :--- | :--- |
| **LLM Model** | `gpt-4o-mini` | 3초 이내의 빠른 응답 속도와 준수한 추론 능력의 균형 |
| **Temperature** | `0.3` | 답변의 일관성을 유지하면서도 자연스러운 문장 생성을 위한 최적점 |
| **Retrieve 개수** | `3개` | 너무 많은 정보는 노이즈가 되고 속도를 저하시킴. 핵심 정보 3개로 충분함 |
| **Threshold** | `30%` | 데이터 양이 많지 않아, 낮은 유사도의 문서라도 LLM이 문맥을 파악하는 데 도움이 됨 |
| **Chat History** | `1개` | *"그건 어떻게 해?"*와 같은 대명사 질문 처리를 위해 직전 대화 1개만 참조 (속도 최적화) |

### 2-3. 검색 품질 향상 (QA 데이터셋 도입 효과)
초기에는 룰북 원문만 사용하여 답변율이 약 60% 수준에 머물렀으나, **QA 데이터셋**을 도입하여 Semantic Gap을 해소한 결과, 답변율과 정확도를 크게 향상시킬 수 있었습니다. 이는 사용자가 묻는 방식(구어체)과 룰북에 적힌 방식(문어체)의 차이를 벡터 공간에서 좁혀주는 역할을 했습니다.

