import json
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.messages import HumanMessage
from langchain_redis import RedisVectorStore
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
import redis

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path="../.env")

rag_document_paths = [
    "./rag_documents/manual.json",
]


class RAGSystem:
    def __init__(
        self,
        redis_url="redis://localhost:6379",
        index_name="rag_index",
        llm_provider=None,
    ):
        """
        RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”

        Args:
            redis_url: Redis ì„œë²„ URL
            index_name: ë²¡í„° ì¸ë±ìŠ¤ ì´ë¦„
            llm_provider: LLM ì œê³µì ('openai' ë˜ëŠ” 'gemini'). Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´
        """
        self.redis_url = os.getenv("REDIS_URL", redis_url)
        self.index_name = index_name
        self.llm_provider = llm_provider or os.getenv("LLM_PROVIDER", "gemini").lower()

        # LLM ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_models()

        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )

        # Redis ì—°ê²° í…ŒìŠ¤íŠ¸
        self._test_redis_connection()

        # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self.vectorstore = None
        self._initialize_vectorstore()

    def _initialize_models(self):
        """LLM ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        if self.llm_provider == "gemini":
            print("ğŸ¤– Gemini 2.5 Flash ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-2.0-flash-exp",
                temperature=0.7,
                google_api_key=os.getenv("GOOGLE_API_KEY"),
            )
            # Gemini ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
            self.embeddings = GoogleGenerativeAIEmbeddings(
                model="models/text-embedding-004",
                google_api_key=os.getenv("GOOGLE_API_KEY"),
            )
        elif self.llm_provider == "openai":
            print("ğŸ¤– OpenAI GPT ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.llm = ChatOpenAI(temperature=0.7, model="gpt-4o")
            self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        else:
            raise ValueError(
                f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µìì…ë‹ˆë‹¤: {self.llm_provider}. 'openai' ë˜ëŠ” 'gemini'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
            )

    def _test_redis_connection(self):
        """Redis ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            r = redis.from_url(self.redis_url)
            r.ping()
            print("âœ… Redis ì—°ê²° ì„±ê³µ!")
        except Exception as e:
            print(f"âŒ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
            print("Redis ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            raise

    def _initialize_vectorstore(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        try:
            # ì„ë² ë”© ëª¨ë¸ë³„ë¡œ ë‹¤ë¥¸ ì¸ë±ìŠ¤ ì´ë¦„ ì‚¬ìš©
            provider_suffix = "_openai" if self.llm_provider == "openai" else "_gemini"
            self.index_name = f"{self.index_name}{provider_suffix}"

            # Redis ì—°ê²° í™•ì¸
            r = redis.from_url(self.redis_url)

            # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            index_exists = False
            try:
                # ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸ ì‹œë„
                self.vectorstore = RedisVectorStore.from_existing_index(
                    embedding=self.embeddings,
                    index_name=self.index_name,
                    redis_url=self.redis_url,
                )
                # ê°„ë‹¨í•œ ê²€ìƒ‰ìœ¼ë¡œ ì¸ë±ìŠ¤ ìœ íš¨ì„± í™•ì¸
                self.vectorstore.similarity_search("í…ŒìŠ¤íŠ¸", k=1)
                index_exists = True
                print(f"ê¸°ì¡´ ë²¡í„° ì¸ë±ìŠ¤ '{self.index_name}' ì‚¬ìš©")
            except Exception as e:
                print(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚¬ìš© ë¶ˆê°€: {str(e)}")
                index_exists = False

            if not index_exists:
                print(f"ìƒˆë¡œìš´ ë²¡í„° ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„±")
                # ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ (ì°¨ì› ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°)
                try:
                    r.delete(self.index_name)
                except Exception:
                    pass

                # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
                self.vectorstore = RedisVectorStore.from_texts(
                    texts=["ì´ˆê¸°í™” ìƒ˜í”Œ í…ìŠ¤íŠ¸"],
                    embedding=self.embeddings,
                    index_name=self.index_name,
                    redis_url=self.redis_url,
                )
                print("âœ… ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
            else:
                print("âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì—°ê²° ì™„ë£Œ!")

        except Exception as e:
            print(f"ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒˆë¡œ ìƒì„±
            try:
                self.vectorstore = RedisVectorStore.from_texts(
                    texts=["ì´ˆê¸°í™” ìƒ˜í”Œ í…ìŠ¤íŠ¸"],
                    embedding=self.embeddings,
                    index_name=self.index_name,
                    redis_url=self.redis_url,
                )
                print("âœ… ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
            except Exception as e2:
                print(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e2}")
                raise

    def add_documents(self, documents):
        """
        ë¬¸ì„œë“¤ì„ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€

        Args:
            documents: Document ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë¬¸ì„œë“¤ì„ ì²­í¬ë¡œ ë¶„í• 
            texts = self.text_splitter.split_documents(documents)
            print(f"ë¬¸ì„œë¥¼ {len(texts)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

            # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
            self.vectorstore.add_documents(texts)
            print("âœ… ë¬¸ì„œë“¤ì´ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")

        except Exception as e:
            print(f"ë¬¸ì„œ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def add_texts(self, datas):
        """
        í…ìŠ¤íŠ¸ë“¤ì„ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€

        Args:
            texts: í…ìŠ¤íŠ¸ ë¬¸ìì—´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            metadatas: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
        """
        try:
            # í…ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
            documents = []
            for i, d in enumerate(datas):
                meta = {
                    k: v
                    for k, v in d.items()
                    if k != "content" and k != "source_citations"
                }
                documents.append(Document(page_content=d["content"], metadata=meta))

            self.add_documents(documents)

        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def search_similar_documents(self, query, k=3):
        """
        ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜

        Returns:
            ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        try:
            results = self.vectorstore.similarity_search(query, k=k)
            print(f"'{query}'ì— ëŒ€í•œ {len(results)}ê°œì˜ ìœ ì‚¬ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return results

        except Exception as e:
            print(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def search_with_scores(self, query, k=3):
        """
        ì ìˆ˜ì™€ í•¨ê»˜ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜

        Returns:
            (ë¬¸ì„œ, ì ìˆ˜) íŠœí”Œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        try:
            results = self.vectorstore.similarity_search_with_score(query, k=k)
            print(f"'{query}'ì— ëŒ€í•œ {len(results)}ê°œì˜ ìœ ì‚¬ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return results

        except Exception as e:
            print(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def generate_answer_with_rag(self, question, k=3, include_metadata=True):
        """
        RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€ ìƒì„±

        Args:
            question: ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            include_metadata: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€

        Returns:
            AIê°€ ìƒì„±í•œ ë‹µë³€
        """
        try:
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ì ìˆ˜ì™€ í•¨ê»˜)
            relevant_docs_with_scores = self.search_with_scores(question, k=k)

            if not relevant_docs_with_scores:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆìŒ)
            context_parts = []
            for i, (doc, score) in enumerate(relevant_docs_with_scores):
                # ê´€ë ¨ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ë²ˆí˜¸ ë§¤ê¸°ê¸°
                context_part = (
                    f"[ì°¸ê³ ìë£Œ {i + 1}] (ê´€ë ¨ë„: {score:.3f})\n{doc.page_content}"
                )

                # ë©”íƒ€ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì¶”ê°€
                if include_metadata and doc.metadata:
                    metadata_str = ", ".join(
                        [f"{k}: {v}" for k, v in doc.metadata.items()]
                    )
                    context_part += f"\n(ì¶œì²˜: {metadata_str})"

                context_parts.append(context_part)

            context = "\n\n" + "\n\n".join(context_parts) + "\n\n"

            # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¹ì‹ ì€ 'ì‚¬ë³´íƒ€ì§€' ê²Œì„ ê·œì¹™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:

## ì»¨í…ìŠ¤íŠ¸ ì •ë³´:
{context}

## ë‹µë³€ ì§€ì¹¨:
0. ì‚¬ìš©ìëŠ” 'ë¬´ì¡°ê±´' ì‚¬ë³´íƒ€ì§€ ê´€ë ¨ ì§ˆë¬¸ë§Œ í•  ê²ƒì…ë‹ˆë‹¤
1. ìœ„ì˜ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
2. ê´€ë ¨ë„ê°€ ë†’ì€ ì°¸ê³ ìë£Œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ì¡°í•´ì£¼ì„¸ìš”.
3. ê·œì¹™ì´ ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš°, ê°€ëŠ¥í•œ í•´ì„ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
4. ë‹µë³€ ì‹œ ì°¸ê³ í•œ ìë£Œ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”. (ì˜ˆ: [ì°¸ê³ ìë£Œ 1] ê¸°ì¤€ìœ¼ë¡œ...)
5. ê°„ê²°í•˜ê²Œ, í•œë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•˜ë„ë¡ í•´.

## ì§ˆë¬¸:
{question}

## ë‹µë³€:"""

            # AI ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
            message = HumanMessage(content=prompt)
            response = self.llm.invoke([message])

            # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
            print(
                f"\n[DEBUG] ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ({len(relevant_docs_with_scores)}ê°œ ë¬¸ì„œ):"
            )
            for i, (doc, score) in enumerate(relevant_docs_with_scores):
                print(
                    f"  {i + 1}. ê´€ë ¨ë„: {score:.3f}, ê¸¸ì´: {len(doc.page_content)}ì"
                )
                if doc.metadata:
                    print(f"     ë©”íƒ€ë°ì´í„°: {doc.metadata}")

            return response.content

        except Exception as e:
            print(f"RAG ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def generate_answer_with_enhanced_context(
        self, question, k=3, similarity_threshold=0.7
    ):
        """
        í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€ ìƒì„±

        Args:
            question: ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ì´ ê°’ ì´ìƒì¸ ë¬¸ì„œë§Œ ì‚¬ìš©)

        Returns:
            AIê°€ ìƒì„±í•œ ë‹µë³€
        """
        try:
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ì ìˆ˜ì™€ í•¨ê»˜)
            relevant_docs_with_scores = self.search_with_scores(
                question, k=k * 2
            )  # ë” ë§ì´ ê²€ìƒ‰í•´ì„œ í•„í„°ë§

            if not relevant_docs_with_scores:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ìœ ì‚¬ë„ ì„ê³„ê°’ìœ¼ë¡œ í•„í„°ë§
            filtered_docs = [
                (doc, score)
                for doc, score in relevant_docs_with_scores
                if score >= similarity_threshold
            ]

            if not filtered_docs:
                # ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ìƒìœ„ kê°œ ì‚¬ìš©
                filtered_docs = relevant_docs_with_scores[:k]
                print(
                    f"[INFO] ì„ê³„ê°’ {similarity_threshold}ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œê°€ ì—†ì–´ ìƒìœ„ {k}ê°œ ë¬¸ì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )
            else:
                # ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œ ì¤‘ ìƒìœ„ kê°œë§Œ ì‚¬ìš©
                filtered_docs = filtered_docs[:k]

            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            high_relevance_docs = [doc for doc, score in filtered_docs if score >= 0.8]
            medium_relevance_docs = [
                doc for doc, score in filtered_docs if 0.6 <= score < 0.8
            ]

            context_parts = []

            # ê³ ê´€ë ¨ë„ ë¬¸ì„œë“¤
            if high_relevance_docs:
                context_parts.append("## í•µì‹¬ ê´€ë ¨ ì •ë³´:")
                for i, doc in enumerate(high_relevance_docs):
                    score = next(score for d, score in filtered_docs if d == doc)
                    context_parts.append(f"### í•µì‹¬ ìë£Œ {i + 1} (ê´€ë ¨ë„: {score:.3f})")
                    context_parts.append(doc.page_content)
                    if doc.metadata:
                        metadata_str = ", ".join(
                            [f"{k}: {v}" for k, v in doc.metadata.items()]
                        )
                        context_parts.append(f"*ì¶œì²˜: {metadata_str}*")

            # ì¤‘ê´€ë ¨ë„ ë¬¸ì„œë“¤
            if medium_relevance_docs:
                context_parts.append("\n## ì¶”ê°€ ì°¸ê³  ì •ë³´:")
                for i, doc in enumerate(medium_relevance_docs):
                    score = next(score for d, score in filtered_docs if d == doc)
                    context_parts.append(f"### ì°¸ê³  ìë£Œ {i + 1} (ê´€ë ¨ë„: {score:.3f})")
                    context_parts.append(doc.page_content)
                    if doc.metadata:
                        metadata_str = ", ".join(
                            [f"{k}: {v}" for k, v in doc.metadata.items()]
                        )
                        context_parts.append(f"*ì¶œì²˜: {metadata_str}*")

            context = "\n\n".join(context_parts)

            # System messageì™€ User messageë¡œ ë¶„ë¦¬í•˜ì—¬ ë” ëª…í™•í•œ ì§€ì‹œ
            system_prompt = """ë‹¹ì‹ ì€ 'ì‚¬ë³´íƒ€ì§€' ê²Œì„ ê·œì¹™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:

0. ì‚¬ìš©ìëŠ” 'ë¬´ì¡°ê±´' ì‚¬ë³´íƒ€ì§€ ê´€ë ¨ ì§ˆë¬¸ë§Œ í•  ê²ƒì…ë‹ˆë‹¤
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”
2. í•µì‹¬ ê´€ë ¨ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ì¡°í•˜ì„¸ìš”
3. ê·œì¹™ì´ ëª…í™•í•˜ì§€ ì•Šê±°ë‚˜ ì—¬ëŸ¬ í•´ì„ì´ ê°€ëŠ¥í•œ ê²½ìš°, ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤ë“¤ì„ ì„¤ëª…í•˜ì„¸ìš”
4. ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ì •ë³´ì˜ ê´€ë ¨ë„ë¥¼ ê³ ë ¤í•˜ì—¬ í™•ì‹ ì˜ ì •ë„ë¥¼ í‘œí˜„í•˜ì„¸ìš”"""

            user_prompt = f"""## ì»¨í…ìŠ¤íŠ¸ ì •ë³´:
{context}

## ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ìœ„ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

            # AI ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„± (ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ìœ ì € ë©”ì‹œì§€ ê²°í•©)
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            response = self.llm.invoke([HumanMessage(content=full_prompt)])

            # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
            print("\n[DEBUG] í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©:")
            print(f"  - ì´ ê²€ìƒ‰ ë¬¸ì„œ: {len(relevant_docs_with_scores)}ê°œ")
            print(f"  - ì„ê³„ê°’ í•„í„°ë§ í›„: {len(filtered_docs)}ê°œ")
            print(f"  - ê³ ê´€ë ¨ë„ ë¬¸ì„œ: {len(high_relevance_docs)}ê°œ")
            print(f"  - ì¤‘ê´€ë ¨ë„ ë¬¸ì„œ: {len(medium_relevance_docs)}ê°œ")

            return response.content

        except Exception as e:
            print(f"í–¥ìƒëœ RAG ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def clear_vectorstore(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        try:
            r = redis.from_url(self.redis_url)
            # í˜„ì¬ ì œê³µìì— ë§ëŠ” ì¸ë±ìŠ¤ ì´ë¦„ ìƒì„±
            provider_suffix = "_openai" if self.llm_provider == "openai" else "_gemini"
            current_index_name = f"rag_index{provider_suffix}"

            r.delete(current_index_name)
            print(f"âœ… ë²¡í„° ì¸ë±ìŠ¤ '{current_index_name}'ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ìƒˆë¡œ ì´ˆê¸°í™”
            self._initialize_vectorstore()

        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")

    def flush_redis_data(self):
        """Redis ì „ì²´ ë°ì´í„° flush (ëª¨ë“  ë°ì´í„° ì‚­ì œ)"""
        try:
            r = redis.from_url(self.redis_url)
            r.flushall()
            print("âœ… Redis ì „ì²´ ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ë²¡í„° ìŠ¤í† ì–´ ì¬ì´ˆê¸°í™”
            print("ğŸ”„ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì¬ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
            self._initialize_vectorstore()

        except Exception as e:
            print(f"âŒ Redis ë°ì´í„° flush ì¤‘ ì˜¤ë¥˜: {e}")

    def init_vectorstore_with_documents(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” í›„ ë¬¸ì„œë“¤ ìë™ ë¡œë“œ"""
        try:
            print("ğŸ”„ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
            self.clear_vectorstore()

            print("ğŸ“– ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€ ì¤‘...")
            sample_datas = []
            for path in rag_document_paths:
                if os.path.exists(path):
                    with open(path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        sample_datas.extend(data)
                        print(f"  - {path}: {len(data)}ê°œ ë¬¸ì„œ ë¡œë“œ")
                else:
                    print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

            if sample_datas:
                self.add_texts(sample_datas)
                print(
                    f"âœ… ì´ {len(sample_datas)}ê°œ ë¬¸ì„œê°€ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!"
                )
            else:
                print("âš ï¸ ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ë° ë¬¸ì„œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

    def get_vectorstore_info(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ì¡°íšŒ"""
        try:
            r = redis.from_url(self.redis_url)

            # ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
            provider_suffix = "_openai" if self.llm_provider == "openai" else "_gemini"
            current_index_name = f"rag_index{provider_suffix}"

            print(f"ğŸ“Š ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ({current_index_name}):")
            print(f"  - Redis URL: {self.redis_url}")
            print(f"  - LLM Provider: {self.llm_provider.upper()}")
            print(f"  - ì¸ë±ìŠ¤ ì´ë¦„: {current_index_name}")

            # Redis í‚¤ ê°œìˆ˜ í™•ì¸
            all_keys = r.keys("*")
            print(f"  - ì´ Redis í‚¤ ê°œìˆ˜: {len(all_keys)}")

            # ë²¡í„° ìŠ¤í† ì–´ ê´€ë ¨ í‚¤ í™•ì¸
            vector_keys = [
                key for key in all_keys if current_index_name.encode() in key
            ]
            print(f"  - ë²¡í„° ìŠ¤í† ì–´ ê´€ë ¨ í‚¤: {len(vector_keys)}")

            if vector_keys:
                print("  - ê´€ë ¨ í‚¤ ëª©ë¡:")
                for key in vector_keys[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                    print(f"    * {key.decode('utf-8')}")
                if len(vector_keys) > 5:
                    print(f"    ... ë° {len(vector_keys) - 5}ê°œ ë”")

        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")


def create_rag_system_with_provider_selection():
    """ì‚¬ìš©ìê°€ LLM ì œê³µìë¥¼ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    print("=== LLM ì œê³µì ì„ íƒ ===")
    print("1. OpenAI (GPT-4o-mini)")
    print("2. Google Gemini (2.5 Flash)")

    current_provider = os.getenv("LLM_PROVIDER", "openai").lower()
    print(f"í˜„ì¬ í™˜ê²½ë³€ìˆ˜ ì„¤ì •: {current_provider.upper()}")

    choice = input("ì‚¬ìš©í•  LLMì„ ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2, ì—”í„°: í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©): ").strip()

    if choice == "1":
        provider = "openai"
    elif choice == "2":
        provider = "gemini"
    else:
        provider = current_provider

    return RAGSystem(llm_provider=provider)


def main():
    """ë©”ì¸ í•¨ìˆ˜ - RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    try:
        print("=== ë³´ë“œê²Œì„ RAG ì‹œìŠ¤í…œ ===")

        # ì‚¬ìš©ìê°€ LLM ì œê³µìë¥¼ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ í•¨
        rag = create_rag_system_with_provider_selection()

        print("-" * 50)
        print("=== ì´ˆê¸°í™” ì˜µì…˜ ===")
        print("1: ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©")
        print("2: ë²¡í„° ìŠ¤í† ì–´ë§Œ ì´ˆê¸°í™” (í˜„ì¬ LLM ì œê³µì)")
        print("3: Redis ì „ì²´ ë°ì´í„° flush (ëª¨ë“  ë°ì´í„° ì‚­ì œ)")
        print("4: ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ì¡°íšŒ")
        print("5: ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” + ë¬¸ì„œ ìë™ ë¡œë“œ")

        init_choice = input("ì„ íƒí•˜ì„¸ìš” (1-5, ê¸°ë³¸ê°’: 1): ").strip() or "1"

        if init_choice == "2":
            print("\nğŸ”„ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
            rag.clear_vectorstore()
        elif init_choice == "3":
            confirm = (
                input("\nâš ï¸ Redis ì „ì²´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
                .strip()
                .lower()
            )
            if confirm == "y":
                rag.flush_redis_data()
            else:
                print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        elif init_choice == "4":
            print("\nğŸ“Š ë²¡í„° ìŠ¤í† ì–´ ì •ë³´:")
            rag.get_vectorstore_info()
        elif init_choice == "5":
            rag.init_vectorstore_with_documents()
        else:
            print("ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        print("-" * 50)

        # ë¬¸ì„œ ì¶”ê°€ ì˜µì…˜ (1ë²ˆ ë˜ëŠ” 5ë²ˆì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
        if init_choice not in ["1", "5"]:
            add_docs = input("\në¬¸ì„œë¥¼ ì¶”ê°€í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if add_docs == "y":
                print("ìƒ˜í”Œ ë¬¸ì„œë“¤ì„ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€í•©ë‹ˆë‹¤...")
                sample_datas = []
                for path in rag_document_paths:
                    if os.path.exists(path):
                        with open(path, "r", encoding="utf-8") as f:
                            data = json.load(f)
                            sample_datas.extend(data)
                    else:
                        print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

                if sample_datas:
                    rag.add_texts(sample_datas)

        # ì§ˆë¬¸ê³¼ ë‹µë³€ í…ŒìŠ¤íŠ¸
        questions = ["ë‚´ ì†ì— ë“¤ë ¤ìˆëŠ” ì¹´ë“œëŠ” ëª‡ì¥ì„ ìœ ì§€í•´ì•¼ í• ê¹Œ?"]

        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì‹¤í–‰ ì—¬ë¶€
        run_test = input("\ní…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if run_test == "y":
            print("\n=== ê¸°ë³¸ RAG í…ŒìŠ¤íŠ¸ ===")
            for question in questions[:1]:  # ì²« ë²ˆì§¸ ì§ˆë¬¸ë§Œ í…ŒìŠ¤íŠ¸
                print(f"\nì§ˆë¬¸: {question}")
                answer = rag.generate_answer_with_rag(question)
                print(f"ë‹µë³€: {answer}")
                print("-" * 50)

            print("\n=== í–¥ìƒëœ RAG í…ŒìŠ¤íŠ¸ ===")
            for question in questions[:1]:  # ì²« ë²ˆì§¸ ì§ˆë¬¸ë§Œ í…ŒìŠ¤íŠ¸
                print(f"\nì§ˆë¬¸: {question}")
                answer = rag.generate_answer_with_enhanced_context(question)
                print(f"ë‹µë³€: {answer}")
                print("-" * 50)

        # get questions from stdin
        print("\n=== ëŒ€í™”í˜• ëª¨ë“œ ===")
        print("ì‚¬ìš©í•  ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1: ê¸°ë³¸ RAG")
        print("2: í–¥ìƒëœ RAG")
        method_choice = input("ì„ íƒ (1 ë˜ëŠ” 2, ê¸°ë³¸ê°’: 2): ").strip() or "2"

        print(f"\nğŸ¤– ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {rag.llm_provider.upper()}")
        if rag.llm_provider == "gemini":
            print("   - LLM: Gemini 2.5 Flash")
            print("   - ì„ë² ë”©: Text Embedding 004")
        else:
            print("   - LLM: GPT-4o-mini")
            print("   - ì„ë² ë”©: Text Embedding 3 Small")

        while True:
            user_input = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): ")
            if user_input.lower() == "exit":
                break

            if method_choice == "1":
                answer = rag.generate_answer_with_rag(user_input)
            else:
                answer = rag.generate_answer_with_enhanced_context(user_input)

            print(f"ë‹µë³€: {answer}")
            print("-" * 50)

    except Exception as e:
        print(f"ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print("í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
        if os.getenv("LLM_PROVIDER", "openai").lower() == "gemini":
            print("- GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
        else:
            print("- OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")


if __name__ == "__main__":
    main()
